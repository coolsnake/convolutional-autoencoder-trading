{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kernel import load_min_data, prepare_data, NetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>oi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-13 14:00:00</th>\n",
       "      <td>645</td>\n",
       "      <td>632</td>\n",
       "      <td>632</td>\n",
       "      <td>645</td>\n",
       "      <td>148052</td>\n",
       "      <td>3570070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-13 14:05:00</th>\n",
       "      <td>647</td>\n",
       "      <td>644</td>\n",
       "      <td>639</td>\n",
       "      <td>640</td>\n",
       "      <td>202724</td>\n",
       "      <td>3586682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-13 14:10:00</th>\n",
       "      <td>643</td>\n",
       "      <td>640</td>\n",
       "      <td>636</td>\n",
       "      <td>641</td>\n",
       "      <td>94164</td>\n",
       "      <td>3578016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-13 14:15:00</th>\n",
       "      <td>655</td>\n",
       "      <td>640</td>\n",
       "      <td>634</td>\n",
       "      <td>651</td>\n",
       "      <td>333452</td>\n",
       "      <td>3612632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-13 14:20:00</th>\n",
       "      <td>657</td>\n",
       "      <td>652</td>\n",
       "      <td>639</td>\n",
       "      <td>641</td>\n",
       "      <td>300840</td>\n",
       "      <td>3585022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     high  open  low  close  volume       oi\n",
       "time                                                        \n",
       "2016-05-13 14:00:00   645   632  632    645  148052  3570070\n",
       "2016-05-13 14:05:00   647   644  639    640  202724  3586682\n",
       "2016-05-13 14:10:00   643   640  636    641   94164  3578016\n",
       "2016-05-13 14:15:00   655   640  634    651  333452  3612632\n",
       "2016-05-13 14:20:00   657   652  639    641  300840  3585022"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_end = '2018-05-13 00:00' # we use 2016 - 2018 for training\n",
    "df_train, df_test = load_min_data()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Model\n",
    "> - transform data into images consisting the past 1000 points <br>\n",
    "> - aim to predict the return after 3 periods = 15 mins <br>\n",
    "> - next split training data into training set and validation set (for model selection) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 8682, array shape: (8682, 1000, 6, 1)\n",
      "validation set size: 3721, and array shape: (3721, 1000, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "time_window = 1000\n",
    "forecast_period = 3\n",
    "X_train, X_valid, y_train, y_valid = prepare_data(df_train, time_window, forecast_period)\n",
    "print(f'training set size: {X_train.shape[0]}, array shape: {X_train.shape}')\n",
    "print(f'validation set size: {X_valid.shape[0]}, and array shape: {X_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Convolution AutoEncoder Network\n",
    "> The reommanded number of filters is (64, 32, 16). <br>\n",
    "> If you don't have a GPU-accelerated machine, use small number of filters such as (16, 8, 8). Or try google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input (None, 1000, 6, 1)\n",
      "shape after first conv (None, 1000, 6, 32)\n",
      "shape after first pooling (None, 200, 6, 32)\n",
      "shape after second conv (None, 200, 6, 16)\n",
      "shape after second pooling (None, 40, 6, 16)\n",
      "shape after third conv (None, 40, 6, 8)\n",
      "shape of encoded (None, 8, 6, 8)\n",
      "shape after upsample third pooling (None, 40, 6, 8)\n",
      "shape after decode third conv (None, 40, 6, 8)\n",
      "shape after upsample second pooling (None, 200, 6, 8)\n",
      "shape after decode second conv (None, 200, 6, 16)\n",
      "shape after upsample first pooling (None, 1000, 6, 16)\n",
      "shape after decode first conv (None, 1000, 6, 32)\n",
      "shape after decode to input (None, 1000, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "net = NetModel(X_train, X_valid, y_train, y_valid)\n",
    "conv_window=(3, 3)\n",
    "pooling_window=(5, 1)\n",
    "n_filters=(32, 16, 8)\n",
    "net.build_net(conv_window, pooling_window, n_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AutoEncoder and Examine the Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8682/8682 [==============================] - 174s 20ms/step - loss: 1472.3104\n",
      "Epoch 2/3\n",
      "8682/8682 [==============================] - 206s 24ms/step - loss: 377.1670\n",
      "Epoch 3/3\n",
      "8682/8682 [==============================] - 197s 23ms/step - loss: 355.8918\n"
     ]
    }
   ],
   "source": [
    "epoches = 3\n",
    "batch_size = 64\n",
    "net.train_encoder(epoches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAC7CAYAAABFJnSnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACu5JREFUeJzt3W1sXnUZx/Hfb33Y2jKKyMbGOkXCQ5gikDSLsoTINLoBgUTfAIoxYvpGkmEwCC994VvlDYlZgEgCspAACSE8JjoRQaDjSWYBF4JsDB1zMMY2t7a7fNF2uTfu0tPS/3161e8nadZ7OzvXdbqrv/13ds65HRECAOSxoO4GAADTQ3ADQDIENwAkQ3ADQDIENwAkQ3ADQDIENwAkQ3ADQDIENwAkQ3ADQDLtRXba1ROdi08useuj4sTRovufcMqij4vX2Lu1yB/DJxz6QnfxGh37XLzGof17NHxof/lCx2nv7Y7Opb1Fa5zZtbvo/ie8sWdZ8RoL391fvIYknfHV8t+jC1R+3N7ZPqLde0YrFSqSGJ2LT9Y53/tZiV0fNbz+w6L7n/Djs54tXuPRL59UvIYkvXnL6uI1Tttc/h9xf3vi1uI1mulc2quzf3Nd0RoPXXBH0f1PuOTest+fknTGLc8XryFJmx75c/EaC11+cXXx+n9V3pZTJQCQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMlUCm7b62y/YXub7ZtLNwUAmNyUwW27TdJtktZLWiXpaturSjcGAGiuyop7taRtEfFWRByWtEnSlWXbAgBMpkpwr5C0veH1jvGfAwDUoEpwN7t3Pj6xkT1ge9D24MjB1jyjACjtmLnee6DudgBJ1YJ7h6SVDa/7JO08fqOI2BgR/RHR397VM1v9AbU6Zq57yz+kC6iiSnC/IOks21+y3SnpKkkPlW0LADCZKR95FREjtq+X9LikNkl3RsTW4p0BAJqq9KzCiHhE0iOFewEAVMCdkwCQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMkQ3ACQTJH3nO/4z0EtvfvVErs+6s1zzyu6/wlDpy0vXqPt7NOK15Ck71/0bPEaTz3x9eI1FJ94VA7wf4UVNwAkQ3ADQDIENwAkQ3ADQDIENwAkQ3ADQDIENwAkQ3ADQDIENwAkM2Vw277T9i7br7WiIQDAp6uy4v6dpHWF+wAAVDRlcEfEU5L2tKAXAEAFnOMGgGRmLbhtD9getD14OP47W7sFatU41yN7D9TdDiBpFoM7IjZGRH9E9Hd60WztFqhV41y393bX3Q4giVMlAJBOlcsB75X0rKRzbO+wfV35tgAAk5nyHXAi4upWNAIAqIZTJQCQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMkQ3ACQzJTXcc/E4VO6tPOa80vs+qgji0aK7n/Ca3uWF6+x97unFq8hSfrgYPESH55RZKSOMfqMi9cA5jJW3ACQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMlUeQeclbb/aHvI9lbbG1rRGACguSq3uY1IujEiXrS9WNIW209GxN8L9wYAaGLKFXdEvBcRL45/vk/SkKQVpRsDADQ3rXPctk+XdKGk50o0AwCYWuXgtn2CpPsl3RARHzX59QHbg7YHRw/un80egdo0zvXI3gN1twNIqhjctjs0Ftr3RMQDzbaJiI0R0R8R/W1dPbPZI1Cbxrlu7+2uux1AUrWrSizpDklDEfHr8i0BAD5NlRX3GknXSlpr++Xxj0sL9wUAmMSUlwNGxNOSeHI9AMwR3DkJAMkQ3ACQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMlUeazrtHXsP6JTny/7vJJrfvJM0f1P+MXn/1G8xmU/+k7xGpK0f21X8Rp9j+0uXmP73pHiNYC5jBU3ACRDcANAMgQ3ACRDcANAMgQ3ACRDcANAMgQ3ACRDcANAMlXeumyR7edtv2J7q+1ftqIxAEBzVe6cPCRpbUR8PP6mwU/bfjQi/lq4NwBAE1XeuiwkfTz+smP8I0o2BQCYXKVz3LbbbL8saZekJyPiubJtAQAmUym4I2I0Ii6Q1Cdpte2vHL+N7QHbg7YHh4fLPmAKaJXGuR7Ze6DudgBJ07yqJCI+lLRZ0romv7YxIvojor+jo2eW2gPq1TjX7b3ddbcDSKp2VckS2yeNf94l6VuSXi/dGACguSpXlSyXdJftNo0F/X0R8XDZtgAAk6lyVcmrki5sQS8AgAq4cxIAkiG4ASAZghsAkiG4ASAZghsAkiG4ASAZghsAkiG4ASCZKndOTlsssEa7iuz6qN/+5ZKi+5/wynl9xWscPnN58RqS9M7OtuI1Tr6os3iN4X+Xna3JRFijR8qudQ616IHJPtKaOq2w0OXn4VCMFK9xZBpPy2bFDQDJENwAkAzBDQDJENwAkAzBDQDJENwAkAzBDQDJENwAkEzl4LbdZvsl27xtGQDUaDor7g2Shko1AgCoplJw2+6TdJmk28u2AwCYStUV962SbpI0j55wAAA5TRncti+XtCsitkyx3YDtQduDw8P7Z61BoE6Ncz360YG62wEkVVtxr5F0he23JW2StNb23cdvFBEbI6I/Ivo7OnpmuU2gHo1z3XZid93tAJIqBHdE3BIRfRFxuqSrJP0hIn5QvDMAQFNcxw0AyUzrCeQRsVnS5iKdAAAqYcUNAMkQ3ACQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMlM6zruqoYXWzsvXlhi10ctWfl+0f1PGFj2p+I1fqUfFq8hScuXfVC8xoHOZcVryOVL1GVhi46t7VALCh0ZLV9DUveCzvJFWvB4vQXTGGxW3ACQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMkQ3ACQDMENAMlUugFn/P0m90kalTQSEf0lmwIATG46d05eEhG7i3UCAKiEUyUAkEzV4A5JT9jeYnugZEMAgE9X9VTJmojYaXuppCdtvx4RTzVuMB7oA5LU3vu5WW4TqEfjXHcs6a25G2BMpRV3ROwc/3GXpAclrW6yzcaI6I+I/raentntEqjJMXN9Ynfd7QCSKgS37R7biyc+l/RtSa+VbgwA0FyVUyWnSnrQ9sT2v4+Ix4p2BQCY1JTBHRFvSTq/Bb0AACrgckAASIbgBoBkCG4ASIbgBoBkCG4ASIbgBoBkCG4ASIbgBoBkHBGzv1P7fUn/nMZvOUXSfHnWN8dS3hcjYkmri85grqW5+zWciflyLHP1OCrPdZHgni7bg/PlXXU4FjSaT1/D+XIs8+E4OFUCAMkQ3ACQzFwJ7o11NzCLOBY0mk9fw/lyLOmPY06c4wYAVDdXVtwAgIpqD27b62y/YXub7Zvr7membK+0/UfbQ7a32t5Qd0+fhe022y/ZfrjuXjJirueu+TDbtQa37TZJt0laL2mVpKttr6qzp89gRNKNEXGupK9J+mniY5GkDZKG6m4iI+Z6zks/23WvuFdL2hYRb0XEYUmbJF1Zc08zEhHvRcSL45/v09hgrKi3q5mx3SfpMkm3191LUsz1HDVfZrvu4F4haXvD6x1KPBQTbJ8u6UJJz9XbyYzdKukmSUfqbiQp5nrumhezXXdwu8nPpb7MxfYJku6XdENEfFR3P9Nl+3JJuyJiS929JMZcz0HzabbrDu4dklY2vO6TtLOmXj4z2x0aG+57IuKBuvuZoTWSrrD9tsb+ib/W9t31tpQOcz03zZvZrvU6btvtkt6U9E1J70p6QdI1EbG1tqZmyLYl3SVpT0TcUHc/s8H2NyT9PCIur7uXTJjruS/7bNe64o6IEUnXS3pcY//pcV/G4R63RtK1Gvtb/OXxj0vrbgqtx1yjNO6cBIBk6j7HDQCYJoIbAJIhuAEgGYIbAJIhuAEgGYIbAJIhuAEgGYIbAJL5H26s+wya/iE2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.get_encoded_series()\n",
    "img = X_train[0][:, :, 0].copy()\n",
    "img_c = net.reconstructed_train[0][:, :, 0].copy()\n",
    "img_c = (MinMaxScaler().fit_transform(img_c) * 255).astype('int')\n",
    "\n",
    "b=20\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.imshow(img[6*(b-1):6*b, :])\n",
    "ax2.imshow(img_c[6*(b-1):6*b, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Asset Return Predictor\n",
    "- 'rf' means random forest classifier\n",
    "- 'xgb' is XGBoost classifier\n",
    "- 'n_search' is the number of random grid-search to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.6446671273900023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.15      0.23      1697\n",
      "         1.0       0.55      0.87      0.67      2024\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      3721\n",
      "   macro avg       0.52      0.51      0.45      3721\n",
      "weighted avg       0.52      0.54      0.47      3721\n",
      "\n",
      "145.05216813087463\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "net.train_classifier('rf', n_search=10)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
